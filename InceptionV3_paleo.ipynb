{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d3b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "#from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c39520",
   "metadata": {},
   "source": [
    "## 1. Препроцессинг изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27df02",
   "metadata": {},
   "source": [
    "### 1.1 Очистка папок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fba5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_folder(mydir):\n",
    "    filelist = [f for f in os.listdir(mydir)]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(mydir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir('data/train'):\n",
    "    clear_folder(f'data/train/{i}')\n",
    "    \n",
    "for i in os.listdir('data/test'):\n",
    "    clear_folder(f'data/test/{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e51a76",
   "metadata": {},
   "source": [
    "### 1.2 Выбор изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62dc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = list(glob.glob('data/Aldanella_attleborensis/*Plate_?_?*a.tif'))\n",
    "f = open('data/Aldanella_attleborensis/different_angle.txt', 'rt')\n",
    "diff_angle = f.readlines()\n",
    "for i, pic in enumerate(diff_angle):\n",
    "    diff_angle[i] = f'data/Aldanella_attleborensis\\\\{pic.rstrip()}.tif'\n",
    "for pic in diff_angle:\n",
    "    if pic in data_names:\n",
    "        data_names.remove(pic)\n",
    "f.close()\n",
    "#print(data_names)\n",
    "for i, n in enumerate(range(len(data_names))):\n",
    "    if i <= int(len(data_names)*0.75):\n",
    "        shutil.copy(data_names[n], 'data/train/class_att')\n",
    "    else:\n",
    "        shutil.copy(data_names[n], 'data/test/class_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128736c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = list(glob.glob('data/Aldanella_sibirica_sp_nov/*Plate_?_?*a.tif'))\n",
    "f = open('data/Aldanella_sibirica_sp_nov/different_angle.txt', 'rt')\n",
    "diff_angle = f.readlines()\n",
    "for i, pic in enumerate(diff_angle):\n",
    "    diff_angle[i] = f'data/Aldanella_sibirica_sp_nov\\\\{pic.rstrip()}.tif'\n",
    "for pic in diff_angle:\n",
    "    if pic in data_names:\n",
    "        data_names.remove(pic)\n",
    "f.close()\n",
    "#print(data_names) \n",
    "for i, n in enumerate(range(len(data_names))):\n",
    "    if i <= int(len(data_names)*0.75):\n",
    "        shutil.copy(data_names[n], 'data/train/class_sib')\n",
    "    else:\n",
    "        shutil.copy(data_names[n], 'data/test/class_sib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7a3d2",
   "metadata": {},
   "source": [
    "### 1.3 Создание тензеров из изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd52c0",
   "metadata": {},
   "source": [
    "Не будем пока делать аугментацию (повороты, добавления шума) изображений. Сделаем это при тренировки полной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6cde5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255) # масштабируем именно значения каждого пикселя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da76ee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 images belonging to 2 classes.\n",
      "Found 17 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Пропускаем через свёртучнуя чать сети изображения (как \"train\", так и \"test\"), чтобы выделить массивы наиболее важных признаков\n",
    "\n",
    "train_generator = datagen.flow_from_directory('data/train/', #Важно разложить по папкам объекты классов\n",
    "                                        target_size=(150, 150), #Размер изображения (высота и ширина в пикселях)\n",
    "                                        batch_size=10, #количество изображений, пропускаемых через сеть на каждой итерации,\n",
    "                                        color_mode='rgb',\n",
    "                                        shuffle=False) #не перемешиваем, т.к. эти данные будем прогонять через уже обученную\n",
    "                                                       #сеть. Далее будет удобно работать с индексами классов при загрузке\n",
    "                                                       #массивов numpy для обучения полносвязанных слоёв\n",
    "                                                       # keep data in same order as labels\n",
    "\n",
    "        \n",
    "#Нужно настроить: color_mode=не \"rgb\" (3 канала), а \"grayscale\" (1 канал)\n",
    "#Трансофрмацию валидационных данных не нужно делать\n",
    "test_generator = datagen.flow_from_directory('data/test/',\n",
    "                                              target_size=(150, 150), #разобраться с количеством тестовых\n",
    "                                                                      #нужно, чтобы их было 25%, а не 50%\n",
    "                                              batch_size=10, #количество изображений, пропускаемых через сеть на каждой итерации,\n",
    "                                              color_mode='rgb',\n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ca83e",
   "metadata": {},
   "source": [
    "## 2. Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a4b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=((150, 150, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7c2a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 68ms/step\n",
      "2/2 [==============================] - 0s 144ms/step\n"
     ]
    }
   ],
   "source": [
    "features_train = inc_model.predict(train_generator) #2000 значит, что мы предскажем \n",
    "                                                    #только первые 2000 элементов из папки\n",
    "np.save(open('data/train_binary/bn_features_train.npy', 'wb'), features_train) #'wb' - открыть/создать\n",
    "                                                                               #файл для двоичной записи\n",
    "\n",
    "features_test = inc_model.predict(test_generator) #Настроить количество предсказаний\n",
    "np.save(open('data/test_binary/bn_features_test.npy', 'wb'), features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e673b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open('data/train_binary/bn_features_train.npy', 'rb'))\n",
    "train_labels = np.array([0] * 19 + [1] * 34) \n",
    "\n",
    "test_data = np.load(open('data/test_binary/bn_features_test.npy', 'rb'))\n",
    "test_labels = np.array([0] * 6 + [1] * 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f76d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_model = Sequential()\n",
    "fc_model.add(Flatten(input_shape=train_data.shape[1:])) #Явно не создаём слой ввода (input)\n",
    "                                                        #'Flatten' cжимает тензор на входе до вектора заданной формы\n",
    "fc_model.add(Dense(128, activation='relu', name='dense_one')) #можно попробовать функцию активации 'selu'\n",
    "fc_model.add(Dropout(0.5, name='dropout_one')) #выбрасываем сигналы от половины нейронов\n",
    "fc_model.add(Dense(128, activation='relu', name='dense_two'))\n",
    "fc_model.add(Dropout(0.5, name='dropout_two'))\n",
    "fc_model.add(Dense(128, activation='relu', name='dense_three'))\n",
    "fc_model.add(Dropout(0.5, name='dropout_three'))\n",
    "fc_model.add(Dense(1, activation='sigmoid', name='output')) #использовать 'softmax' для многоклассовой классификации\n",
    "#количество нейронов в выходном слое равно количеству классов (записываем максимальный номер класса)\n",
    "\n",
    "fc_model.compile(optimizer='adam', #поиск минимума (может быть 'adam', 'sgd', 'rmsprop')\n",
    "              loss='binary_crossentropy', # 'categorical_crossentropy'\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3cd2582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 31ms/step - loss: 1.6787 - accuracy: 0.4906 - val_loss: 0.7396 - val_accuracy: 0.4706\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.5238 - accuracy: 0.5660 - val_loss: 0.6471 - val_accuracy: 0.7059\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.0905 - accuracy: 0.5283 - val_loss: 0.5510 - val_accuracy: 0.5882\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.3566 - accuracy: 0.6604 - val_loss: 0.5265 - val_accuracy: 0.7647\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2983 - accuracy: 0.5660 - val_loss: 0.4790 - val_accuracy: 0.7647\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.3238 - accuracy: 0.5849 - val_loss: 0.5415 - val_accuracy: 0.7059\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.9378 - accuracy: 0.6981 - val_loss: 0.5845 - val_accuracy: 0.6471\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.9869 - accuracy: 0.6415 - val_loss: 0.5703 - val_accuracy: 0.7647\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9204 - accuracy: 0.6038 - val_loss: 0.5777 - val_accuracy: 0.7059\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.7756 - accuracy: 0.6415 - val_loss: 0.5682 - val_accuracy: 0.6471\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9903 - accuracy: 0.5660 - val_loss: 0.5372 - val_accuracy: 0.7647\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.7964 - accuracy: 0.5660 - val_loss: 0.6006 - val_accuracy: 0.6471\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.8058 - accuracy: 0.6415 - val_loss: 0.6631 - val_accuracy: 0.6471\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.5896 - accuracy: 0.6604 - val_loss: 0.6443 - val_accuracy: 0.6471\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6046 - accuracy: 0.6415 - val_loss: 0.5911 - val_accuracy: 0.6471\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.5669 - accuracy: 0.6981 - val_loss: 0.5777 - val_accuracy: 0.7647\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5497 - accuracy: 0.6792 - val_loss: 0.5633 - val_accuracy: 0.7647\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.6389 - accuracy: 0.6415 - val_loss: 0.5419 - val_accuracy: 0.7059\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.6020 - accuracy: 0.6226 - val_loss: 0.5435 - val_accuracy: 0.7059\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.5624 - accuracy: 0.6415 - val_loss: 0.5831 - val_accuracy: 0.7647\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.6439 - accuracy: 0.6981 - val_loss: 0.5734 - val_accuracy: 0.7647\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5057 - accuracy: 0.6792 - val_loss: 0.5484 - val_accuracy: 0.7647\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5261 - accuracy: 0.6415 - val_loss: 0.5476 - val_accuracy: 0.7059\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5761 - accuracy: 0.6792 - val_loss: 0.5585 - val_accuracy: 0.7059\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4370 - accuracy: 0.6415 - val_loss: 0.5396 - val_accuracy: 0.7059\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4803 - accuracy: 0.7547 - val_loss: 0.5296 - val_accuracy: 0.7059\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4322 - accuracy: 0.8113 - val_loss: 0.5438 - val_accuracy: 0.7647\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4050 - accuracy: 0.7547 - val_loss: 0.5596 - val_accuracy: 0.7647\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.5135 - accuracy: 0.6981 - val_loss: 0.5344 - val_accuracy: 0.7059\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3876 - accuracy: 0.7925 - val_loss: 0.4778 - val_accuracy: 0.7647\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4864 - accuracy: 0.7170 - val_loss: 0.4729 - val_accuracy: 0.7647\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4099 - accuracy: 0.8679 - val_loss: 0.5151 - val_accuracy: 0.7647\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4328 - accuracy: 0.7547 - val_loss: 0.6444 - val_accuracy: 0.7647\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4220 - accuracy: 0.7358 - val_loss: 0.5278 - val_accuracy: 0.7059\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4755 - accuracy: 0.7925 - val_loss: 0.5176 - val_accuracy: 0.7059\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.4050 - accuracy: 0.8491 - val_loss: 0.5706 - val_accuracy: 0.7059\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3696 - accuracy: 0.8113 - val_loss: 0.6083 - val_accuracy: 0.7647\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3542 - accuracy: 0.8113 - val_loss: 0.6607 - val_accuracy: 0.7059\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.3430 - accuracy: 0.8302 - val_loss: 0.5965 - val_accuracy: 0.7059\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3533 - accuracy: 0.8491 - val_loss: 0.5847 - val_accuracy: 0.7059\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2790 - accuracy: 0.8491 - val_loss: 0.5965 - val_accuracy: 0.7059\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3513 - accuracy: 0.8679 - val_loss: 0.6723 - val_accuracy: 0.7059\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.2998 - accuracy: 0.8491 - val_loss: 0.6925 - val_accuracy: 0.7059\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.2764 - accuracy: 0.8491 - val_loss: 0.6953 - val_accuracy: 0.7059\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.2195 - accuracy: 0.9434 - val_loss: 0.7544 - val_accuracy: 0.7059\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.2444 - accuracy: 0.9245 - val_loss: 0.7666 - val_accuracy: 0.7059\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2694 - accuracy: 0.8679 - val_loss: 0.6889 - val_accuracy: 0.7059\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2960 - accuracy: 0.8868 - val_loss: 0.6098 - val_accuracy: 0.7059\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3289 - accuracy: 0.8868 - val_loss: 0.7527 - val_accuracy: 0.7059\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2590 - accuracy: 0.8679 - val_loss: 1.1285 - val_accuracy: 0.7059\n"
     ]
    }
   ],
   "source": [
    "fc_model.fit(train_data, train_labels,\n",
    "            epochs=50, batch_size=10,\n",
    "            validation_data=(test_data, test_labels))\n",
    "\n",
    "fc_model.save_weights('data/model_weights/first_test.hdf5') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d2ff8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fc_model.evaluate(test_data, test_labels, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "720fd88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1285 - accuracy: 0.7059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.1284996271133423, 'accuracy': 0.7058823704719543}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = fc_model.evaluate(test_data, test_labels)\n",
    "dict(zip(fc_model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087dc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(inc_model.output)\n",
    "x = Dense(128, activation='relu', name='dense_one')(x)\n",
    "x = Dropout(0.5, name='dropout_one')(x)\n",
    "x = Dense(128, activation='relu', name='dense_two')(x)\n",
    "x = Dropout(0.5, name='dropout_two')(x)\n",
    "x = Dense(128, activation='relu', name='dense_three')(x)\n",
    "x = Dropout(0.5, name='dropout_three')(x)\n",
    "top_model=Dense(1, activation='sigmoid', name='output')(x)\n",
    "model = Model(inputs=inc_model.input, outputs=top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ebdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filename='data/model_weights/first_test.hdf5'\n",
    "model.load_weights(weights_filename, by_name=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5020f7da",
   "metadata": {},
   "source": [
    "for layer in inc_model.layers[:205]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1ab715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(learning_rate=1e-4, momentum=0.9),\n",
    "                #optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d621274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 images belonging to 2 classes.\n",
      "Found 17 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\npred_generator=test_datagen.flow_from_directory('data/img_val/',\\n                                                     target_size=(150,150),\\n                                                     batch_size=100,\\n                                                     class_mode='binary')\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=90)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/train/',\n",
    "                                        target_size=(150, 150),\n",
    "                                        batch_size=10,\n",
    "                                        color_mode='rgb',\n",
    "                                        class_mode='binary',\n",
    "                                        shuffle=True)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory('data/test/',\n",
    "                                              target_size=(150, 150),\n",
    "                                              batch_size=10,\n",
    "                                              color_mode='rgb',\n",
    "                                              class_mode='binary',\n",
    "                                              shuffle=True)\n",
    "\n",
    "'''\n",
    "pred_generator=test_datagen.flow_from_directory('data/img_val/',\n",
    "                                                     target_size=(150,150),\n",
    "                                                     batch_size=100,\n",
    "                                                     class_mode='binary')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b459c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, #new_lr = lr * factor\n",
    "                              patience=5, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23a2b2b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6/6 [==============================] - 7s 320ms/step - loss: 0.7512 - accuracy: 0.7170 - val_loss: 0.9814 - val_accuracy: 0.7059\n",
      "Epoch 2/150\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6819 - accuracy: 0.6415 - val_loss: 0.8579 - val_accuracy: 0.7059\n",
      "Epoch 3/150\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.6537 - accuracy: 0.6038 - val_loss: 0.7321 - val_accuracy: 0.7059\n",
      "Epoch 4/150\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.8137 - accuracy: 0.5472 - val_loss: 0.6355 - val_accuracy: 0.7059\n",
      "Epoch 5/150\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6580 - accuracy: 0.5849 - val_loss: 0.5420 - val_accuracy: 0.7059\n",
      "Epoch 6/150\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.5704 - accuracy: 0.6792 - val_loss: 0.4708 - val_accuracy: 0.7059\n",
      "Epoch 7/150\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.5600 - accuracy: 0.7358 - val_loss: 0.4327 - val_accuracy: 0.7059\n",
      "Epoch 8/150\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.4922 - accuracy: 0.6792 - val_loss: 0.4028 - val_accuracy: 0.8235\n",
      "Epoch 9/150\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.5043 - accuracy: 0.6792 - val_loss: 0.3981 - val_accuracy: 0.8824\n",
      "Epoch 10/150\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.6805 - accuracy: 0.6038 - val_loss: 0.3765 - val_accuracy: 0.8824\n",
      "Epoch 11/150\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.5332 - accuracy: 0.7547 - val_loss: 0.3788 - val_accuracy: 0.8824\n",
      "Epoch 12/150\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.5149 - accuracy: 0.7547 - val_loss: 0.3888 - val_accuracy: 0.8824\n",
      "Epoch 13/150\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.6137 - accuracy: 0.7170 - val_loss: 0.4096 - val_accuracy: 0.8235\n",
      "Epoch 14/150\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.5409 - accuracy: 0.6981 - val_loss: 0.4154 - val_accuracy: 0.8235\n",
      "Epoch 15/150\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.5171 - accuracy: 0.7547 - val_loss: 0.4147 - val_accuracy: 0.8235\n",
      "Epoch 16/150\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.5012 - accuracy: 0.7547 - val_loss: 0.4118 - val_accuracy: 0.8824\n",
      "Epoch 17/150\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.5605 - accuracy: 0.7358 - val_loss: 0.4103 - val_accuracy: 0.8824\n",
      "Epoch 18/150\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.4002 - accuracy: 0.8302 - val_loss: 0.4343 - val_accuracy: 0.8235\n",
      "Epoch 19/150\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.4907 - accuracy: 0.7358 - val_loss: 0.4364 - val_accuracy: 0.7647\n",
      "Epoch 20/150\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.4031 - accuracy: 0.8302 - val_loss: 0.4359 - val_accuracy: 0.7059\n",
      "Epoch 21/150\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.3966 - accuracy: 0.7736 - val_loss: 0.4361 - val_accuracy: 0.7059\n",
      "Epoch 22/150\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.4611 - accuracy: 0.7170 - val_loss: 0.4512 - val_accuracy: 0.7059\n",
      "Epoch 23/150\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.4139 - accuracy: 0.8302 - val_loss: 0.4827 - val_accuracy: 0.7059\n",
      "Epoch 24/150\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.4021 - accuracy: 0.8113 - val_loss: 0.4815 - val_accuracy: 0.7647\n",
      "Epoch 25/150\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.4291 - accuracy: 0.7925 - val_loss: 0.4669 - val_accuracy: 0.7059\n",
      "Epoch 26/150\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.3929 - accuracy: 0.8302 - val_loss: 0.4785 - val_accuracy: 0.6471\n",
      "Epoch 27/150\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.4581 - accuracy: 0.7925 - val_loss: 0.4750 - val_accuracy: 0.7059\n",
      "Epoch 28/150\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.3852 - accuracy: 0.8113 - val_loss: 0.4713 - val_accuracy: 0.7059\n",
      "Epoch 29/150\n",
      "6/6 [==============================] - 1s 124ms/step - loss: 0.3813 - accuracy: 0.9057 - val_loss: 0.4935 - val_accuracy: 0.7059\n",
      "Epoch 30/150\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.3367 - accuracy: 0.8302 - val_loss: 0.5041 - val_accuracy: 0.7059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29bad17a5b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        train_generator,\n",
    "        epochs=150,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[early_stopping]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c8460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5041 - accuracy: 0.7059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.5041056871414185, 'accuracy': 0.7058823704719543}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.evaluate_generator(pred_generator)\n",
    "result = model.evaluate(validation_generator)\n",
    "dict(zip(fc_model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b547f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
