{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d3b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "#from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c39520",
   "metadata": {},
   "source": [
    "## 1. Препроцессинг изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27df02",
   "metadata": {},
   "source": [
    "### 1.1 Очистка папок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fba5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_folder(mydir):\n",
    "    filelist = [f for f in os.listdir(mydir)]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(mydir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir('data/train_a'):\n",
    "    clear_folder(f'data/train_a/{i}')\n",
    "    \n",
    "for i in os.listdir('data/test_a'):\n",
    "    clear_folder(f'data/test_a/{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e51a76",
   "metadata": {},
   "source": [
    "### 1.2 Выбор изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62dc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = list(glob.glob('data/Aldanella_attleborensis/*Plate_?*_?*a.tif'))\n",
    "f = open('data/Aldanella_attleborensis/different_angle.txt', 'rt')\n",
    "diff_angle = f.readlines()\n",
    "for i, pic in enumerate(diff_angle):\n",
    "    diff_angle[i] = f'data/Aldanella_attleborensis\\\\{pic.rstrip()}.tif'\n",
    "for pic in diff_angle:\n",
    "    if pic in data_names:\n",
    "        data_names.remove(pic)\n",
    "f.close()\n",
    "#print(data_names)\n",
    "for i, n in enumerate(range(len(data_names))):\n",
    "    if i <= int(len(data_names)*0.75):\n",
    "        shutil.copy(data_names[n], 'data/train_a/class_att')\n",
    "    else:\n",
    "        shutil.copy(data_names[n], 'data/test_a/class_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128736c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = list(glob.glob('data/Aldanella_sibirica_sp_nov/*Plate_?*_?*a.tif'))\n",
    "f = open('data/Aldanella_sibirica_sp_nov/different_angle.txt', 'rt')\n",
    "diff_angle = f.readlines()\n",
    "for i, pic in enumerate(diff_angle):\n",
    "    diff_angle[i] = f'data/Aldanella_sibirica_sp_nov\\\\{pic.rstrip()}.tif'\n",
    "for pic in diff_angle:\n",
    "    if pic in data_names:\n",
    "        data_names.remove(pic)\n",
    "f.close()\n",
    "#print(data_names) \n",
    "for i, n in enumerate(range(len(data_names))):\n",
    "    if i <= int(len(data_names)*0.75):\n",
    "        shutil.copy(data_names[n], 'data/train_a/class_sib')\n",
    "    else:\n",
    "        shutil.copy(data_names[n], 'data/test_a/class_sib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7a3d2",
   "metadata": {},
   "source": [
    "### 1.3 Создание тензеров из изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd52c0",
   "metadata": {},
   "source": [
    "Не будем пока делать аугментацию (повороты, добавления шума) изображений. Сделаем это при тренировки полной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6cde5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255) # масштабируем именно значения каждого пикселя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da76ee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 2 classes.\n",
      "Found 22 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Пропускаем через свёртучнуя чать сети изображения (как \"train\", так и \"test\"), чтобы выделить массивы наиболее важных признаков\n",
    "\n",
    "train_generator = datagen.flow_from_directory('data/train_a/', #Важно разложить по папкам объекты классов\n",
    "                                        target_size=(150, 150), #Размер изображения (высота и ширина в пикселях)\n",
    "                                        batch_size=10, #количество изображений, пропускаемых через сеть на каждой итерации,\n",
    "                                        color_mode='rgb',\n",
    "                                        shuffle=False) #не перемешиваем, т.к. эти данные будем прогонять через уже обученную\n",
    "                                                       #сеть. Далее будет удобно работать с индексами классов при загрузке\n",
    "                                                       #массивов numpy для обучения полносвязанных слоёв\n",
    "                                                       # keep data in same order as labels\n",
    "\n",
    "        \n",
    "#Нужно настроить: color_mode=не \"rgb\" (3 канала), а \"grayscale\" (1 канал)\n",
    "#Трансофрмацию валидационных данных не нужно делать\n",
    "test_generator = datagen.flow_from_directory('data/test_a/',\n",
    "                                              target_size=(150, 150), #разобраться с количеством тестовых\n",
    "                                                                      #нужно, чтобы их было 25%, а не 50%\n",
    "                                              batch_size=10, #количество изображений, пропускаемых через сеть на каждой итерации,\n",
    "                                              color_mode='rgb',\n",
    "                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ca83e",
   "metadata": {},
   "source": [
    "## 2. Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a4b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=((150, 150, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7c2a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 69ms/step\n",
      "3/3 [==============================] - 0s 87ms/step\n"
     ]
    }
   ],
   "source": [
    "features_train = inc_model.predict(train_generator) #2000 значит, что мы предскажем \n",
    "                                                    #только первые 2000 элементов из папки\n",
    "np.save(open('data/binary/train_a_binary/bn_features_exp1_train_a.npy', 'wb'), features_train) #'wb' - открыть/создать\n",
    "                                                                               #файл для двоичной записи\n",
    "\n",
    "features_test = inc_model.predict(test_generator) #Настроить количество предсказаний\n",
    "np.save(open('data/binary/test_a_binary/bn_features_exp1_test_a.npy', 'wb'), features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e673b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open('data/binary/train_a_binary/bn_features_exp1_train_a.npy', 'rb'))\n",
    "train_labels = np.array([0] * 19 + [1] * 52) \n",
    "\n",
    "test_data = np.load(open('data/binary/test_a_binary/bn_features_exp1_test_a.npy', 'rb'))\n",
    "test_labels = np.array([0] * 6 + [1] * 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f76d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_model = Sequential()\n",
    "fc_model.add(Flatten(input_shape=train_data.shape[1:])) #Явно не создаём слой ввода (input)\n",
    "                                                        #'Flatten' cжимает тензор на входе до вектора заданной формы\n",
    "fc_model.add(Dense(128, activation='relu', name='dense_one')) #можно попробовать функцию активации 'selu'\n",
    "fc_model.add(Dropout(0.5, name='dropout_one')) #выбрасываем сигналы от половины нейронов\n",
    "fc_model.add(Dense(128, activation='relu', name='dense_two'))\n",
    "fc_model.add(Dropout(0.5, name='dropout_two'))\n",
    "fc_model.add(Dense(128, activation='relu', name='dense_three'))\n",
    "fc_model.add(Dropout(0.5, name='dropout_three'))\n",
    "fc_model.add(Dense(1, activation='sigmoid', name='output')) #использовать 'softmax' для многоклассовой классификации\n",
    "#количество нейронов в выходном слое равно количеству классов (записываем максимальный номер класса)\n",
    "\n",
    "fc_model.compile(optimizer='adam', #поиск минимума (может быть 'adam', 'sgd', 'rmsprop')\n",
    "              loss='binary_crossentropy', # 'categorical_crossentropy'\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3cd2582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 1.3155 - accuracy: 0.6338 - val_loss: 0.6081 - val_accuracy: 0.7273\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2581 - accuracy: 0.7465 - val_loss: 0.6154 - val_accuracy: 0.7273\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.7863 - accuracy: 0.6197 - val_loss: 0.5662 - val_accuracy: 0.7273\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.1288 - accuracy: 0.6479 - val_loss: 0.5811 - val_accuracy: 0.7727\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.7881 - accuracy: 0.6197 - val_loss: 0.5452 - val_accuracy: 0.7273\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9952 - accuracy: 0.6761 - val_loss: 0.5654 - val_accuracy: 0.7273\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9312 - accuracy: 0.6338 - val_loss: 0.5426 - val_accuracy: 0.7273\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.8730 - accuracy: 0.6338 - val_loss: 0.5055 - val_accuracy: 0.7273\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.7254 - accuracy: 0.6901 - val_loss: 0.5320 - val_accuracy: 0.7727\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7037 - accuracy: 0.6901 - val_loss: 0.5545 - val_accuracy: 0.7727\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5548 - accuracy: 0.7183 - val_loss: 0.5578 - val_accuracy: 0.7273\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5757 - accuracy: 0.7465 - val_loss: 0.5716 - val_accuracy: 0.7727\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5232 - accuracy: 0.7324 - val_loss: 0.5529 - val_accuracy: 0.7727\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5830 - accuracy: 0.7042 - val_loss: 0.5474 - val_accuracy: 0.7273\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5405 - accuracy: 0.6901 - val_loss: 0.5425 - val_accuracy: 0.7273\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5045 - accuracy: 0.7183 - val_loss: 0.5433 - val_accuracy: 0.7273\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4731 - accuracy: 0.6901 - val_loss: 0.6128 - val_accuracy: 0.7273\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5415 - accuracy: 0.6761 - val_loss: 0.5533 - val_accuracy: 0.7727\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4622 - accuracy: 0.7606 - val_loss: 0.5681 - val_accuracy: 0.7273\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4116 - accuracy: 0.7606 - val_loss: 0.5839 - val_accuracy: 0.7273\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4541 - accuracy: 0.7324 - val_loss: 0.5278 - val_accuracy: 0.8182\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4594 - accuracy: 0.8310 - val_loss: 0.5102 - val_accuracy: 0.7727\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5537 - accuracy: 0.7465 - val_loss: 0.5610 - val_accuracy: 0.7273\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3576 - accuracy: 0.8451 - val_loss: 0.5889 - val_accuracy: 0.7273\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3942 - accuracy: 0.7606 - val_loss: 0.6049 - val_accuracy: 0.7727\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4491 - accuracy: 0.8169 - val_loss: 0.6549 - val_accuracy: 0.7273\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4314 - accuracy: 0.7324 - val_loss: 0.6258 - val_accuracy: 0.7727\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3771 - accuracy: 0.8310 - val_loss: 0.7453 - val_accuracy: 0.7273\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3984 - accuracy: 0.7746 - val_loss: 0.7315 - val_accuracy: 0.7273\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4099 - accuracy: 0.8169 - val_loss: 0.6811 - val_accuracy: 0.7273\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4685 - accuracy: 0.8169 - val_loss: 0.6065 - val_accuracy: 0.7273\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4181 - accuracy: 0.8169 - val_loss: 0.8050 - val_accuracy: 0.7273\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4381 - accuracy: 0.7324 - val_loss: 0.7347 - val_accuracy: 0.7273\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3956 - accuracy: 0.7887 - val_loss: 0.7600 - val_accuracy: 0.7273\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4005 - accuracy: 0.7887 - val_loss: 0.7066 - val_accuracy: 0.7727\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4359 - accuracy: 0.7606 - val_loss: 0.7709 - val_accuracy: 0.7273\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3892 - accuracy: 0.8169 - val_loss: 0.7079 - val_accuracy: 0.7727\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4190 - accuracy: 0.8169 - val_loss: 0.6236 - val_accuracy: 0.7273\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4962 - accuracy: 0.6338 - val_loss: 0.5450 - val_accuracy: 0.7727\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4597 - accuracy: 0.7324 - val_loss: 0.5696 - val_accuracy: 0.8182\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4328 - accuracy: 0.7324 - val_loss: 0.6369 - val_accuracy: 0.7273\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4051 - accuracy: 0.7465 - val_loss: 0.5967 - val_accuracy: 0.7273\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4119 - accuracy: 0.7887 - val_loss: 0.6098 - val_accuracy: 0.7273\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4229 - accuracy: 0.7324 - val_loss: 0.6363 - val_accuracy: 0.7273\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4396 - accuracy: 0.7465 - val_loss: 0.7747 - val_accuracy: 0.7273\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4296 - accuracy: 0.7606 - val_loss: 0.5780 - val_accuracy: 0.8182\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4607 - accuracy: 0.7183 - val_loss: 0.5930 - val_accuracy: 0.8182\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5104 - accuracy: 0.7042 - val_loss: 0.7013 - val_accuracy: 0.7273\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4051 - accuracy: 0.7746 - val_loss: 0.8288 - val_accuracy: 0.7273\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4066 - accuracy: 0.7746 - val_loss: 0.8217 - val_accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "fc_model.fit(train_data, train_labels,\n",
    "            epochs=50, batch_size=10,\n",
    "            validation_data=(test_data, test_labels))\n",
    "\n",
    "fc_model.save_weights('data/model_weights/exp1_a.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d2ff8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fc_model.evaluate(test_data, test_labels, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "720fd88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8217 - accuracy: 0.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.8217106461524963, 'accuracy': 0.7272727489471436}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = fc_model.evaluate(test_data, test_labels)\n",
    "dict(zip(fc_model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087dc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(inc_model.output)\n",
    "x = Dense(128, activation='relu', name='dense_one')(x)\n",
    "x = Dropout(0.5, name='dropout_one')(x)\n",
    "x = Dense(128, activation='relu', name='dense_two')(x)\n",
    "x = Dropout(0.5, name='dropout_two')(x)\n",
    "x = Dense(128, activation='relu', name='dense_three')(x)\n",
    "x = Dropout(0.5, name='dropout_three')(x)\n",
    "top_model=Dense(1, activation='sigmoid', name='output')(x)\n",
    "model = Model(inputs=inc_model.input, outputs=top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ebdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filename='data/model_weights/exp1_a.hdf5'\n",
    "model.load_weights(weights_filename, by_name=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5020f7da",
   "metadata": {},
   "source": [
    "for layer in inc_model.layers[:205]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1ab715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(learning_rate=1e-4, momentum=0.9),\n",
    "                #optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d621274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 2 classes.\n",
      "Found 22 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\npred_generator=test_datagen.flow_from_directory('data/img_val/',\\n                                                     target_size=(150,150),\\n                                                     batch_size=100,\\n                                                     class_mode='binary')\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=90)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/train_a/',\n",
    "                                        target_size=(150, 150),\n",
    "                                        batch_size=10,\n",
    "                                        color_mode='rgb',\n",
    "                                        class_mode='binary',\n",
    "                                        shuffle=True)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory('data/test_a/',\n",
    "                                              target_size=(150, 150),\n",
    "                                              batch_size=10,\n",
    "                                              color_mode='rgb',\n",
    "                                              class_mode='binary',\n",
    "                                              shuffle=True)\n",
    "\n",
    "'''\n",
    "pred_generator=test_datagen.flow_from_directory('data/img_val/',\n",
    "                                                     target_size=(150,150),\n",
    "                                                     batch_size=100,\n",
    "                                                     class_mode='binary')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b459c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, #new_lr = lr * factor\n",
    "                              patience=5, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23a2b2b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "8/8 [==============================] - 7s 259ms/step - loss: 0.5572 - accuracy: 0.7324 - val_loss: 0.7732 - val_accuracy: 0.7727\n",
      "Epoch 2/150\n",
      "8/8 [==============================] - 1s 111ms/step - loss: 0.5512 - accuracy: 0.6338 - val_loss: 0.7199 - val_accuracy: 0.7727\n",
      "Epoch 3/150\n",
      "8/8 [==============================] - 1s 105ms/step - loss: 0.6131 - accuracy: 0.6901 - val_loss: 0.6728 - val_accuracy: 0.7727\n",
      "Epoch 4/150\n",
      "8/8 [==============================] - 1s 106ms/step - loss: 0.5591 - accuracy: 0.6901 - val_loss: 0.5923 - val_accuracy: 0.7727\n",
      "Epoch 5/150\n",
      "8/8 [==============================] - 1s 105ms/step - loss: 0.6457 - accuracy: 0.6901 - val_loss: 0.5228 - val_accuracy: 0.8182\n",
      "Epoch 6/150\n",
      "8/8 [==============================] - 1s 109ms/step - loss: 0.5209 - accuracy: 0.7183 - val_loss: 0.4898 - val_accuracy: 0.8182\n",
      "Epoch 7/150\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 0.6283 - accuracy: 0.5775 - val_loss: 0.4694 - val_accuracy: 0.7273\n",
      "Epoch 8/150\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.6616 - accuracy: 0.6761 - val_loss: 0.4379 - val_accuracy: 0.7727\n",
      "Epoch 9/150\n",
      "8/8 [==============================] - 1s 110ms/step - loss: 0.5250 - accuracy: 0.7324 - val_loss: 0.4202 - val_accuracy: 0.7727\n",
      "Epoch 10/150\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.5123 - accuracy: 0.6901 - val_loss: 0.4127 - val_accuracy: 0.7727\n",
      "Epoch 11/150\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 0.6060 - accuracy: 0.6761 - val_loss: 0.4174 - val_accuracy: 0.7727\n",
      "Epoch 12/150\n",
      "8/8 [==============================] - 1s 111ms/step - loss: 0.5913 - accuracy: 0.6338 - val_loss: 0.4089 - val_accuracy: 0.7727\n",
      "Epoch 13/150\n",
      "8/8 [==============================] - 1s 112ms/step - loss: 0.5427 - accuracy: 0.6761 - val_loss: 0.4204 - val_accuracy: 0.7727\n",
      "Epoch 14/150\n",
      "8/8 [==============================] - 1s 106ms/step - loss: 0.5147 - accuracy: 0.7465 - val_loss: 0.4466 - val_accuracy: 0.7727\n",
      "Epoch 15/150\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.6057 - accuracy: 0.6620 - val_loss: 0.4700 - val_accuracy: 0.7727\n",
      "Epoch 16/150\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.5301 - accuracy: 0.6761 - val_loss: 0.4971 - val_accuracy: 0.7727\n",
      "Epoch 17/150\n",
      "8/8 [==============================] - 1s 106ms/step - loss: 0.5445 - accuracy: 0.6620 - val_loss: 0.5101 - val_accuracy: 0.7727\n",
      "Epoch 18/150\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.5510 - accuracy: 0.6338 - val_loss: 0.5178 - val_accuracy: 0.7273\n",
      "Epoch 19/150\n",
      "8/8 [==============================] - 1s 113ms/step - loss: 0.5386 - accuracy: 0.6620 - val_loss: 0.5284 - val_accuracy: 0.7273\n",
      "Epoch 20/150\n",
      "8/8 [==============================] - 1s 106ms/step - loss: 0.4789 - accuracy: 0.6761 - val_loss: 0.5357 - val_accuracy: 0.7273\n",
      "Epoch 21/150\n",
      "8/8 [==============================] - 1s 106ms/step - loss: 0.4984 - accuracy: 0.6761 - val_loss: 0.5546 - val_accuracy: 0.7273\n",
      "Epoch 22/150\n",
      "8/8 [==============================] - 1s 105ms/step - loss: 0.4867 - accuracy: 0.7465 - val_loss: 0.5481 - val_accuracy: 0.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c5a96fd90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        train_generator,\n",
    "        epochs=150,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[early_stopping]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c8460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5479 - accuracy: 0.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.5479055643081665, 'accuracy': 0.7272727489471436}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.evaluate_generator(pred_generator)\n",
    "result = model.evaluate(validation_generator)\n",
    "dict(zip(fc_model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b547f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
